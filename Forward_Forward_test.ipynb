{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOR+RTCPpf5ekXlGdcOoDIT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiGyt/snippets/blob/master/Forward_Forward_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUmzXAG4SwS8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import random\n",
        "from tensorflow.compiler.tf2xla.python import xla\n",
        "import tensorflow_probability as tfp\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "print(\"4 Random Training samples and labels\")\n",
        "idx1, idx2, idx3, idx4 = random.sample(range(0, x_train.shape[0]), 4)\n",
        "\n",
        "img1 = (x_train[idx1], y_train[idx1])\n",
        "img2 = (x_train[idx2], y_train[idx2])\n",
        "img3 = (x_train[idx3], y_train[idx3])\n",
        "img4 = (x_train[idx4], y_train[idx4])\n",
        "\n",
        "imgs = [img1, img2, img3, img4]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "for idx, item in enumerate(imgs):\n",
        "    image, label = item[0], item[1]\n",
        "    plt.subplot(2, 2, idx + 1)\n",
        "    plt.imshow(image, cmap=\"gray\")\n",
        "    plt.title(f\"Label : {label}\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "wjky_BM44YfD",
        "outputId": "68a6e1dc-002f-47f1-a808-c07b69e631b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 Random Training samples and labels\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preproc dataset\n",
        "\n",
        "x_train = x_train.astype(float) / 255\n",
        "x_test = x_test.astype(float) / 255\n",
        "y_train = y_train.astype(int)\n",
        "y_test = y_test.astype(int)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "\n",
        "train_dataset = train_dataset.batch(60000)\n",
        "test_dataset = test_dataset.batch(10000)\n"
      ],
      "metadata": {
        "id": "YebsyRK54oud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a ff dense\n",
        "class FFDense(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom ForwardForward-enabled Dense layer. It has an implementation of the\n",
        "    Forward-Forward network internally for use.\n",
        "    This layer must be used in conjunction with the `FFNetwork` model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        units,\n",
        "        optimizer,\n",
        "        loss_metric,\n",
        "        num_epochs=50,\n",
        "        use_bias=True,\n",
        "        kernel_initializer=\"glorot_uniform\",\n",
        "        bias_initializer=\"zeros\",\n",
        "        kernel_regularizer=None,\n",
        "        bias_regularizer=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = keras.layers.Dense(\n",
        "            units=units,\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=kernel_regularizer,\n",
        "            bias_regularizer=bias_regularizer,\n",
        "        )\n",
        "        self.relu = keras.layers.ReLU()\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_metric = loss_metric\n",
        "        self.threshold = 1.5\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "    # We perform a normalization step before we run the input through the Dense\n",
        "    # layer.\n",
        "\n",
        "    def call(self, x):\n",
        "        x_norm = tf.norm(x, ord=2, axis=1, keepdims=True)\n",
        "        x_norm = x_norm + 1e-4\n",
        "        x_dir = x / x_norm\n",
        "        res = self.dense(x_dir)\n",
        "        return self.relu(res)\n",
        "\n",
        "    # The Forward-Forward algorithm is below. We first perform the Dense-layer\n",
        "    # operation and then get a Mean Square value for all positive and negative\n",
        "    # samples respectively.\n",
        "    # The custom loss function finds the distance between the Mean-squared\n",
        "    # result and the threshold value we set (a hyperparameter) that will define\n",
        "    # whether the prediction is positive or negative in nature. Once the loss is\n",
        "    # calculated, we get a mean across the entire batch combined and perform a\n",
        "    # gradient calculation and optimization step. This does not technically\n",
        "    # qualify as backpropagation since there is no gradient being\n",
        "    # sent to any previous layer and is completely local in nature.\n",
        "\n",
        "    def forward_forward(self, x_pos, x_neg):\n",
        "        for i in range(self.num_epochs):\n",
        "            with tf.GradientTape() as tape:\n",
        "                g_pos = tf.math.reduce_mean(tf.math.pow(self.call(x_pos), 2), 1)\n",
        "                g_neg = tf.math.reduce_mean(tf.math.pow(self.call(x_neg), 2), 1)\n",
        "\n",
        "                loss = tf.math.log(\n",
        "                    1\n",
        "                    + tf.math.exp(\n",
        "                        tf.concat([-g_pos + self.threshold, g_neg - self.threshold], 0)\n",
        "                    )\n",
        "                )\n",
        "                mean_loss = tf.cast(tf.math.reduce_mean(loss), tf.float32)\n",
        "                self.loss_metric.update_state([mean_loss])\n",
        "            gradients = tape.gradient(mean_loss, self.dense.trainable_weights)\n",
        "            self.optimizer.apply_gradients(zip(gradients, self.dense.trainable_weights))\n",
        "        return (\n",
        "            tf.stop_gradient(self.call(x_pos)),\n",
        "            tf.stop_gradient(self.call(x_neg)),\n",
        "            self.loss_metric.result(),\n",
        "        )\n"
      ],
      "metadata": {
        "id": "_RZS1vYr4ZsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a full ff network\n",
        "class FFNetwork(keras.Model):\n",
        "    \"\"\"\n",
        "    A [`keras.Model`](/api/models/model#model-class) that supports a `FFDense` network creation. This model\n",
        "    can work for any kind of classification task. It has an internal\n",
        "    implementation with some details specific to the MNIST dataset which can be\n",
        "    changed as per the use-case.\n",
        "    \"\"\"\n",
        "\n",
        "    # Since each layer runs gradient-calculation and optimization locally, each\n",
        "    # layer has its own optimizer that we pass. As a standard choice, we pass\n",
        "    # the `Adam` optimizer with a default learning rate of 0.03 as that was\n",
        "    # found to be the best rate after experimentation.\n",
        "    # Loss is tracked using `loss_var` and `loss_count` variables.\n",
        "    # Use legacy optimizer for Layer Optimizer to fix issue\n",
        "    # https://github.com/keras-team/keras-io/issues/1241\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dims,\n",
        "        layer_optimizer=keras.optimizers.legacy.Adam(learning_rate=0.03),\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layer_optimizer = layer_optimizer\n",
        "        self.loss_var = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
        "        self.loss_count = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
        "        self.layer_list = [keras.Input(shape=(dims[0],))]\n",
        "        for d in range(len(dims) - 1):\n",
        "            self.layer_list += [\n",
        "                FFDense(\n",
        "                    dims[d + 1],\n",
        "                    optimizer=self.layer_optimizer,\n",
        "                    loss_metric=keras.metrics.Mean(),\n",
        "                )\n",
        "            ]\n",
        "\n",
        "    # This function makes a dynamic change to the image wherein the labels are\n",
        "    # put on top of the original image (for this example, as MNIST has 10\n",
        "    # unique labels, we take the top-left corner's first 10 pixels). This\n",
        "    # function returns the original data tensor with the first 10 pixels being\n",
        "    # a pixel-based one-hot representation of the labels.\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def overlay_y_on_x(self, data):\n",
        "        X_sample, y_sample = data\n",
        "        max_sample = tf.reduce_max(X_sample, axis=0, keepdims=True)\n",
        "        max_sample = tf.cast(max_sample, dtype=tf.float64)\n",
        "        X_zeros = tf.zeros([10], dtype=tf.float64)\n",
        "        X_update = xla.dynamic_update_slice(X_zeros, max_sample, [y_sample])\n",
        "        X_sample = xla.dynamic_update_slice(X_sample, X_update, [0])\n",
        "        return X_sample, y_sample\n",
        "\n",
        "    # A custom `predict_one_sample` performs predictions by passing the images\n",
        "    # through the network, measures the results produced by each layer (i.e.\n",
        "    # how high/low the output values are with respect to the set threshold for\n",
        "    # each label) and then simply finding the label with the highest values.\n",
        "    # In such a case, the images are tested for their 'goodness' with all\n",
        "    # labels.\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def predict_one_sample(self, x):\n",
        "        goodness_per_label = []\n",
        "        x = tf.reshape(x, [tf.shape(x)[0] * tf.shape(x)[1]])\n",
        "        for label in range(10):\n",
        "            h, label = self.overlay_y_on_x(data=(x, label))\n",
        "            h = tf.reshape(h, [-1, tf.shape(h)[0]])\n",
        "            goodness = []\n",
        "            for layer_idx in range(1, len(self.layer_list)):\n",
        "                layer = self.layer_list[layer_idx]\n",
        "                h = layer(h)\n",
        "                goodness += [tf.math.reduce_mean(tf.math.pow(h, 2), 1)]\n",
        "            goodness_per_label += [\n",
        "                tf.expand_dims(tf.reduce_sum(goodness, keepdims=True), 1)\n",
        "            ]\n",
        "        goodness_per_label = tf.concat(goodness_per_label, 1)\n",
        "        return tf.cast(tf.argmax(goodness_per_label, 1), tf.float64)\n",
        "\n",
        "    def predict(self, data):\n",
        "        x = data\n",
        "        preds = list()\n",
        "        preds = tf.map_fn(fn=self.predict_one_sample, elems=x)\n",
        "        return np.asarray(preds, dtype=int)\n",
        "\n",
        "    # This custom `train_step` function overrides the internal `train_step`\n",
        "    # implementation. We take all the input image tensors, flatten them and\n",
        "    # subsequently produce positive and negative samples on the images.\n",
        "    # A positive sample is an image that has the right label encoded on it with\n",
        "    # the `overlay_y_on_x` function. A negative sample is an image that has an\n",
        "    # erroneous label present on it.\n",
        "    # With the samples ready, we pass them through each `FFLayer` and perform\n",
        "    # the Forward-Forward computation on it. The returned loss is the final\n",
        "    # loss value over all the layers.\n",
        "\n",
        "    @tf.function(jit_compile=True)\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        # Flatten op\n",
        "        x = tf.reshape(x, [-1, tf.shape(x)[1] * tf.shape(x)[2]])\n",
        "\n",
        "        x_pos, y = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, y))\n",
        "\n",
        "        random_y = tf.random.shuffle(y)\n",
        "        x_neg, y = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, random_y))\n",
        "\n",
        "        h_pos, h_neg = x_pos, x_neg\n",
        "\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            if isinstance(layer, FFDense):\n",
        "                print(f\"Training layer {idx+1} now : \")\n",
        "                h_pos, h_neg, loss = layer.forward_forward(h_pos, h_neg)\n",
        "                self.loss_var.assign_add(loss)\n",
        "                self.loss_count.assign_add(1.0)\n",
        "            else:\n",
        "                print(f\"Passing layer {idx+1} now : \")\n",
        "                x = layer(x)\n",
        "        mean_res = tf.math.divide(self.loss_var, self.loss_count)\n",
        "        return {\"FinalLoss\": mean_res}\n"
      ],
      "metadata": {
        "id": "TwLPbwhL4fN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FFNetwork(dims=[784, 500, 500])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.03),\n",
        "    loss=\"mse\",\n",
        "    jit_compile=True,\n",
        "    metrics=[keras.metrics.Mean()],\n",
        ")\n",
        "\n",
        "epochs = 250\n",
        "history = model.fit(train_dataset, epochs=epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwaxyFkF4lHh",
        "outputId": "f26a8403-909b-4d52-f550-6c5a64afa23d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "Training layer 1 now : \n",
            "Training layer 2 now : \n",
            "Training layer 1 now : \n",
            "Training layer 2 now : \n",
            "1/1 [==============================] - 101s 101s/step - FinalLoss: 0.7279\n",
            "Epoch 2/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.7082\n",
            "Epoch 3/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.7022\n",
            "Epoch 4/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.6796\n",
            "Epoch 5/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.6565\n",
            "Epoch 6/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.6347\n",
            "Epoch 7/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.6152\n",
            "Epoch 8/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5983\n",
            "Epoch 9/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5831\n",
            "Epoch 10/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5697\n",
            "Epoch 11/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5576\n",
            "Epoch 12/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5468\n",
            "Epoch 13/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5368\n",
            "Epoch 14/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5278\n",
            "Epoch 15/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5195\n",
            "Epoch 16/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5119\n",
            "Epoch 17/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5049\n",
            "Epoch 18/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4985\n",
            "Epoch 19/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4925\n",
            "Epoch 20/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4869\n",
            "Epoch 21/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4816\n",
            "Epoch 22/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4767\n",
            "Epoch 23/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4721\n",
            "Epoch 24/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4677\n",
            "Epoch 25/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4636\n",
            "Epoch 26/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4597\n",
            "Epoch 27/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4560\n",
            "Epoch 28/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4525\n",
            "Epoch 29/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4491\n",
            "Epoch 30/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4458\n",
            "Epoch 31/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4428\n",
            "Epoch 32/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4398\n",
            "Epoch 33/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4371\n",
            "Epoch 34/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4344\n",
            "Epoch 35/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4319\n",
            "Epoch 36/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4295\n",
            "Epoch 37/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4271\n",
            "Epoch 38/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4249\n",
            "Epoch 39/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4228\n",
            "Epoch 40/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4206\n",
            "Epoch 41/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4186\n",
            "Epoch 42/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4166\n",
            "Epoch 43/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4147\n",
            "Epoch 44/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4129\n",
            "Epoch 45/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4111\n",
            "Epoch 46/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4094\n",
            "Epoch 47/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4078\n",
            "Epoch 48/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4061\n",
            "Epoch 49/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4046\n",
            "Epoch 50/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4031\n",
            "Epoch 51/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4016\n",
            "Epoch 52/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4001\n",
            "Epoch 53/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3987\n",
            "Epoch 54/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3974\n",
            "Epoch 55/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3960\n",
            "Epoch 56/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3947\n",
            "Epoch 57/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3934\n",
            "Epoch 58/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3922\n",
            "Epoch 59/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3909\n",
            "Epoch 60/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3898\n",
            "Epoch 61/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3887\n",
            "Epoch 62/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3875\n",
            "Epoch 63/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3864\n",
            "Epoch 64/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3853\n",
            "Epoch 65/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3842\n",
            "Epoch 66/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3832\n",
            "Epoch 67/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3822\n",
            "Epoch 68/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3811\n",
            "Epoch 69/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3802\n",
            "Epoch 70/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3792\n",
            "Epoch 71/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3782\n",
            "Epoch 72/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3773\n",
            "Epoch 73/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3764\n",
            "Epoch 74/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3755\n",
            "Epoch 75/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3746\n",
            "Epoch 76/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3737\n",
            "Epoch 77/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3729\n",
            "Epoch 78/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3720\n",
            "Epoch 79/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3712\n",
            "Epoch 80/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3704\n",
            "Epoch 81/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3696\n",
            "Epoch 82/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3688\n",
            "Epoch 83/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3680\n",
            "Epoch 84/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3673\n",
            "Epoch 85/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3665\n",
            "Epoch 86/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3658\n",
            "Epoch 87/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3651\n",
            "Epoch 88/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3643\n",
            "Epoch 89/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3636\n",
            "Epoch 90/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3629\n",
            "Epoch 91/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3623\n",
            "Epoch 92/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3616\n",
            "Epoch 93/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3610\n",
            "Epoch 94/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3603\n",
            "Epoch 95/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3597\n",
            "Epoch 96/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3591\n",
            "Epoch 97/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3585\n",
            "Epoch 98/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3578\n",
            "Epoch 99/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3572\n",
            "Epoch 100/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3566\n",
            "Epoch 101/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3560\n",
            "Epoch 102/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3554\n",
            "Epoch 103/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3549\n",
            "Epoch 104/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3543\n",
            "Epoch 105/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3537\n",
            "Epoch 106/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3532\n",
            "Epoch 107/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3526\n",
            "Epoch 108/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3521\n",
            "Epoch 109/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3515\n",
            "Epoch 110/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3510\n",
            "Epoch 111/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3505\n",
            "Epoch 112/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3500\n",
            "Epoch 113/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3495\n",
            "Epoch 114/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3490\n",
            "Epoch 115/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3485\n",
            "Epoch 116/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3480\n",
            "Epoch 117/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3475\n",
            "Epoch 118/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3470\n",
            "Epoch 119/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3466\n",
            "Epoch 120/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3461\n",
            "Epoch 121/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3457\n",
            "Epoch 122/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3452\n",
            "Epoch 123/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3447\n",
            "Epoch 124/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3443\n",
            "Epoch 125/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3439\n",
            "Epoch 126/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3434\n",
            "Epoch 127/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3430\n",
            "Epoch 128/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3426\n",
            "Epoch 129/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3422\n",
            "Epoch 130/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3418\n",
            "Epoch 131/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3414\n",
            "Epoch 132/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3409\n",
            "Epoch 133/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3406\n",
            "Epoch 134/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3402\n",
            "Epoch 135/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3398\n",
            "Epoch 136/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3394\n",
            "Epoch 137/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3390\n",
            "Epoch 138/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3386\n",
            "Epoch 139/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3383\n",
            "Epoch 140/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3379\n",
            "Epoch 141/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3375\n",
            "Epoch 142/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3372\n",
            "Epoch 143/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3368\n",
            "Epoch 144/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3364\n",
            "Epoch 145/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3361\n",
            "Epoch 146/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3357\n",
            "Epoch 147/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3354\n",
            "Epoch 148/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3351\n",
            "Epoch 149/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3347\n",
            "Epoch 150/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3344\n",
            "Epoch 151/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3341\n",
            "Epoch 152/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3337\n",
            "Epoch 153/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3334\n",
            "Epoch 154/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3331\n",
            "Epoch 155/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3328\n",
            "Epoch 156/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3325\n",
            "Epoch 157/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3322\n",
            "Epoch 158/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3319\n",
            "Epoch 159/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3316\n",
            "Epoch 160/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3313\n",
            "Epoch 161/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3310\n",
            "Epoch 162/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3307\n",
            "Epoch 163/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3304\n",
            "Epoch 164/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3301\n",
            "Epoch 165/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3298\n",
            "Epoch 166/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3295\n",
            "Epoch 167/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3292\n",
            "Epoch 168/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3290\n",
            "Epoch 169/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3287\n",
            "Epoch 170/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3284\n",
            "Epoch 171/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3281\n",
            "Epoch 172/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3279\n",
            "Epoch 173/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3276\n",
            "Epoch 174/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3273\n",
            "Epoch 175/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3271\n",
            "Epoch 176/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3268\n",
            "Epoch 177/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3265\n",
            "Epoch 178/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3263\n",
            "Epoch 179/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3260\n",
            "Epoch 180/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3258\n",
            "Epoch 181/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3255\n",
            "Epoch 182/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3253\n",
            "Epoch 183/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3250\n",
            "Epoch 184/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3248\n",
            "Epoch 185/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3245\n",
            "Epoch 186/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3243\n",
            "Epoch 187/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3241\n",
            "Epoch 188/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3238\n",
            "Epoch 189/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3236\n",
            "Epoch 190/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3234\n",
            "Epoch 191/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3231\n",
            "Epoch 192/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3229\n",
            "Epoch 193/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3227\n",
            "Epoch 194/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3225\n",
            "Epoch 195/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3223\n",
            "Epoch 196/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3220\n",
            "Epoch 197/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3218\n",
            "Epoch 198/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3216\n",
            "Epoch 199/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3214\n",
            "Epoch 200/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3212\n",
            "Epoch 201/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3210\n",
            "Epoch 202/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3208\n",
            "Epoch 203/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3206\n",
            "Epoch 204/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3204\n",
            "Epoch 205/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3201\n",
            "Epoch 206/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3199\n",
            "Epoch 207/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3197\n",
            "Epoch 208/250\n",
            "1/1 [==============================] - 7s 7s/step - FinalLoss: 0.3195\n",
            "Epoch 209/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3193\n",
            "Epoch 210/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3191\n",
            "Epoch 211/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3190\n",
            "Epoch 212/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3188\n",
            "Epoch 213/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3186\n",
            "Epoch 214/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3184\n",
            "Epoch 215/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3182\n",
            "Epoch 216/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3180\n",
            "Epoch 217/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3178\n",
            "Epoch 218/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3176\n",
            "Epoch 219/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3174\n",
            "Epoch 220/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3173\n",
            "Epoch 221/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3171\n",
            "Epoch 222/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3169\n",
            "Epoch 223/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3167\n",
            "Epoch 224/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3165\n",
            "Epoch 225/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3164\n",
            "Epoch 226/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3162\n",
            "Epoch 227/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3160\n",
            "Epoch 228/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3158\n",
            "Epoch 229/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3157\n",
            "Epoch 230/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3155\n",
            "Epoch 231/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3153\n",
            "Epoch 232/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3152\n",
            "Epoch 233/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3150\n",
            "Epoch 234/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3148\n",
            "Epoch 235/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3147\n",
            "Epoch 236/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3145\n",
            "Epoch 237/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3143\n",
            "Epoch 238/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3142\n",
            "Epoch 239/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3140\n",
            "Epoch 240/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3139\n",
            "Epoch 241/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3137\n",
            "Epoch 242/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3136\n",
            "Epoch 243/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3134\n",
            "Epoch 244/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3133\n",
            "Epoch 245/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3131\n",
            "Epoch 246/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3129\n",
            "Epoch 247/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3128\n",
            "Epoch 248/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3126\n",
            "Epoch 249/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3125\n",
            "Epoch 250/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inference and testing\n",
        "preds = model.predict(tf.convert_to_tensor(x_test))\n",
        "\n",
        "preds = preds.reshape((preds.shape[0], preds.shape[1]))\n",
        "\n",
        "results = accuracy_score(preds, y_test)\n",
        "\n",
        "print(f\"Test Accuracy score : {results*100}%\")\n",
        "\n",
        "plt.plot(range(len(history.history[\"FinalLoss\"])), history.history[\"FinalLoss\"])\n",
        "plt.title(\"Loss over training\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "AY00w-Pa_DFV",
        "outputId": "7eb154c7-383a-40ee-d26c-64fe1e25758c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy score : 97.72999999999999%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAiUElEQVR4nO3deXSc9X3v8fdX62hfrMWSLXkBgzGbMcYmBEgCTcFZoGlPCYRmaUNockKT3JCckCY35XKTk+WmbW4bmkJaeskCJC0kuA1LSgKxIWCQwRgbG7wv8iLJ2iVr/94/5rEydiRLliU9mmc+r3PmzDzP/DTz/emxP/PT71nG3B0REUl+aWEXICIik0OBLiISEQp0EZGIUKCLiESEAl1EJCIU6CIiEaFAFwmJmT1uZh+e7LaSukzHocvpMrPdwC3u/lTYtUwXM3NgkbtvD7sWkWM0Qhc5CTPLmM6fEzkdCnSZMmaWbWbfMbMDwe07ZpYdPFdmZv9lZq1m1mxma80sLXjuC2ZWb2YdZvaGmV09yusXmdkPzKzRzPaY2ZfNLC1431YzOy+hbbmZHTWzimD5PWa2IWj3WzO7IKHt7qCGjUDXieFsZmuCh6+aWaeZvd/M3m5m+4OfOwT8m5mVBH1sNLOW4PHchNd5xsxuCR5/xMyeNbNvB213mdmqCbZdYGZrgt/fU2Z2t5n9aIKbUZKIAl2m0peAS4GlwIXACuDLwXO3A/uBcqAS+GvAzexs4DbgEncvAK4Bdo/y+v8IFAELgbcBHwL+3N17gUeAmxLa3gD8xt0bzOwi4D7gL4FZwD3A6mMfNoGbgHcDxe4+kPim7n5l8PBCd893958Ey7OBUmAecCvx/1//FizXAkeB747+62Il8AZQBnwL+Fczswm0fQB4MejbncAHT/KeEiEKdJlKNwN3uXuDuzcC/4vfhUs/UAXMc/d+d1/r8R06g0A2sMTMMt19t7vvOPGFzSwduBH4ort3uPtu4G8TXv+B4PljPhCsg3jY3uPu69x90N3vB3qJf/gc8w/uvs/dj55Cf4eAv3H3Xnc/6u5H3P1hd+929w7ga8Q/eEazx92/7+6DwP3B76fyVNqaWS1wCfAVd+9z92eB1afQB0liCnSZStXAnoTlPcE6gP8DbAd+aWY7zewOgGAn42eIjywbzOwhM6vm95UBmSO8/pzg8dNArpmtNLP5xP9K+Fnw3Dzg9mC6pdXMWoGahNoA9p1qZ4FGd+85tmBmuWZ2TzAd1A6sAYqDD6ORHDr2wN27g4f5p9i2GmhOWAcT64skIQW6TKUDxMPzmNpgHcGo+nZ3XwhcB3z22Fy5uz/g7pcHP+vAN0d47Sbio/wTX78+eI1B4KfEp05uAv4rGCVDPOC+5u7FCbdcd38w4bUmcvjXiT9zO3A2sNLdC4FjUzWjTaNMhoNAqZnlJqyrmcL3kxlEgS6TJdPMYgm3DOBB4MvBDsky4CvAj2B4p+SZwbxvG/GpliEzO9vMrgrms3uIzzsPnfhmCYH9NTMrMLN5wGePvX7gAeD9xKd+HkhY/33g48Ho3cwsz8zebWYFp9Dfw8Tn7k+mIKi/1cxKgb85hdefEHffA9QBd5pZlpm9BXjvVL+vzAwKdJksjxEPr2O3O4GvEg+XjcBrwMvBOoBFwFNAJ/A88E/u/jTx+fNvEB+BHwIqgC+O8p5/BXQBO4FniYf2fceedPd1wfPVwOMJ6+uAjxHfQdlCfOrnI6fY3zuB+4MpmxtGafMdICfoywvAE6f4HhN1M/AW4Ajx3/dPiO8jkIjTiUUiEWdmPwG2uvuU/4Ug4dIIXSRizOwSMzsjOCb/WuB64OchlyXTQGeziUTPbOLH4c8ifqz/J9z9lXBLkumgKRcRkYjQlIuISESENuVSVlbm8+fPD+vtRUSS0vr165vcvXyk50IL9Pnz51NXVxfW24uIJCUz2zPac5pyERGJCAW6iEhEKNBFRCJCgS4iEhEKdBGRiFCgi4hEhAJdRCQiki7Q63Y3880ntqJLFoiIHC/pAn3j/ja+98wOmrv6wi5FRGRGSbpAry2Nf7PW3ubuMVqKiKSWpAv0miDQ97Wcypexi4hEXxIGeg4A+zRCFxE5TtIFem5WBmX5WQp0EZETJF2gQ3zaZV+LAl1EJFFyBnpJrnaKioicICkDvbY0lwOtPQwMDoVdiojIjJGUgV5TmsPgkHOwrSfsUkREZoykDPTa0jwAXj/YHnIlIiIzx7gC3cyuNbM3zGy7md0xwvN/b2YbgtubZtY66ZUmuHheCeUF2Tywbu9Uvo2ISFIZM9DNLB24G1gFLAFuMrMliW3c/X+4+1J3Xwr8I/DIFNQ6LCsjjZtX1vKbNxvZ2dg5lW8lIpI0xjNCXwFsd/ed7t4HPARcf5L2NwEPTkZxJ/OBlbUA/GLjwal+KxGRpDCeQJ8D7EtY3h+s+z1mNg9YAPz69Es7uYqCGOUF2ToeXUQkMNk7RW8E/sPdB0d60sxuNbM6M6trbGw87TerLorpSBcRkcB4Ar0eqElYnhusG8mNnGS6xd3vdffl7r68vLx8/FWOoqoohwOtukiXiAiML9BfAhaZ2QIzyyIe2qtPbGRmi4ES4PnJLXF0VcXxEbq+7EJEZByB7u4DwG3Ak8AW4KfuvtnM7jKz6xKa3gg85NOYrnOKc+juG6T96MB0vaWIyIyVMZ5G7v4Y8NgJ675ywvKdk1fW+FQVxS+lW996lKLczOl+exGRGSUpzxQ9pqo4BsDBNs2ji4gkdaBXByP0AzrSRUQkuQO9vCCbjDTjoI50ERFJ7kBPTzMqC2M6dFFEhCQPdIDq4pimXEREiECgVxXlaKeoiAgRCPTq4hwOtfUwNKSTi0QktUUg0GP0DzpNXb1hlyIiEqqkD/RjJxcdaNU8uoiktggEenBykY50EZEUl/SBPqdYJxeJiEAEAr04N5NYZppG6CKS8pI+0M2M6qIcDujQRRFJcUkf6BC/SJd2iopIqotEoFfr5CIRkWgEelVxDg0dvfQPDoVdiohIaCIR6NVFMdzhkI50EZEUFolArwoOXTyoQBeRFBaJQJ+jby4SEYlGoOv0fxGRiAR6XnYGhbEMjdBFJKVFItAhfhldfXORiKSyiAW6plxEJHVFJtCrimKachGRlBaZQK8uzqGlu5+jfYNhlyIiEooIBXr80MV6zaOLSIqKTKDXlOQCsL+lO+RKRETCEZ1AL40H+r5mBbqIpKbIBHp5fjbZGWnsVaCLSIqKTKCnpRk1pbkKdBFJWZEJdIDa0lz2NmunqIikpkgFek1JDvubu3H3sEsREZl20Qr00lw6egdo7e4PuxQRkWkXqUCvDY500Ty6iKSiaAX6rHig71Ggi0gKilagByP03U1dIVciIjL9IhXouVkZVBfF2NnYGXYpIiLTLlKBDrCwPJ9dGqGLSAqKYKDnsbOxS4cuikjKGVegm9m1ZvaGmW03sztGaXODmb1uZpvN7IHJLXP8Fpbl0dE7QGNnb1gliIiEImOsBmaWDtwNvBPYD7xkZqvd/fWENouALwJvdfcWM6uYqoLHsqA8H4CdjV1UFMTCKkNEZNqNZ4S+Atju7jvdvQ94CLj+hDYfA+529xYAd2+Y3DLHb2FZHhAPdBGRVDKeQJ8D7EtY3h+sS3QWcJaZPWdmL5jZtSO9kJndamZ1ZlbX2Ng4sYrHKrY4h+yMNB3pIiIpZ7J2imYAi4C3AzcB3zez4hMbufu97r7c3ZeXl5dP0lsfLy3NWFiez3YFuoikmPEEej1Qk7A8N1iXaD+w2t373X0X8CbxgA/F2ZX5vHmoI6y3FxEJxXgC/SVgkZktMLMs4EZg9Qltfk58dI6ZlRGfgtk5eWWemkWVBRxo66GjRxfpEpHUMWagu/sAcBvwJLAF+Km7bzazu8zsuqDZk8ARM3sdeBr4vLsfmaqix3JWZQEA2xo07SIiqWPMwxYB3P0x4LET1n0l4bEDnw1uoTurMn7o4puHOlhWWxJyNSIi0yNyZ4oC1JTkEstM483DGqGLSOqIZKCnpRmLKgp487B2jIpI6ohkoAOcPbuArYfadU0XEUkZkQ30c6sLaerso6FD13QRkdQQ4UAvAmDzgbaQKxERmR6RDfRzquKHLm6ubw+5EhGR6RHZQC+IZTJ/Vi6bDyjQRSQ1RDbQIT7tsvmgplxEJDVEOtCXVBeyr/kord19YZciIjLlIh3oS2uKAdi4X6N0EYm+SAf6+XOLMIMN+1rDLkVEZMpFOtALY5mcUZ7Pqwp0EUkBkQ50iE+7bNjXqjNGRSTyIh/oF9YUc6Srj/0tR8MuRURkSkU+0JfVFgOwfk9LuIWIiEyxyAf64tmFFMQyWLcrtO/bEBGZFpEP9PQ0Y8X8UtbtbA67FBGRKRX5QAdYubCUnU1dNLT3hF2KiMiUSY1AXzALgHW7NEoXkehKiUA/t7qQ/GzNo4tItKVEoGekp7F8fonm0UUk0lIi0CE+7bKtoZOmTn2DkYhEU+oE+sJSAF7UPLqIRFTKBPr5c4rIzUpn3U7No4tINKVMoGemp3HJ/FLWbmsKuxQRkSmRMoEOcPU5Fexs6mJnY2fYpYiITLqUCvR3nF0BwK+3NoRciYjI5EupQK8pzeXsygJ+tUWBLiLRk1KBDvFpl5d2N9N2tD/sUkREJlVKBvrAkLPmzcawSxERmVQpF+hLa0ooyc3UPLqIRE7KBXp6mvGOsyt4+o0GBgaHwi5HRGTSpFygA1x9TiWt3f06a1REIiUlA/2qxRXkZqWz+tUDYZciIjJpUjLQc7LSuebc2Tz22kF6BwbDLkdEZFKkZKADXLe0mvaeAX7zho52EZFoSNlAv/zMMkrzsnhU0y4iEhEpG+iZ6Wm854Iqnnr9MJ29A2GXIyJy2sYV6GZ2rZm9YWbbzeyOEZ7/iJk1mtmG4HbL5Jc6+a5fWk3vwBC/3Hwo7FJERE7bmIFuZunA3cAqYAlwk5ktGaHpT9x9aXD7l0muc0osqy2hpjSHn9btC7sUEZHTNp4R+gpgu7vvdPc+4CHg+qkta3qYGTetqOWFnc1sb+gIuxwRkdMynkCfAyQOYfcH6070J2a20cz+w8xqJqW6afD+5TVkpafxoxf2hl2KiMhpmaydov8JzHf3C4D/Bu4fqZGZ3WpmdWZW19g4Mw4XnJWfzbsvqOLh9fvp0s5REUli4wn0eiBxxD03WDfM3Y+4e2+w+C/AxSO9kLvf6+7L3X15eXn5ROqdEn926Tw6egd4dIMOYRSR5DWeQH8JWGRmC8wsC7gRWJ3YwMyqEhavA7ZMXolTb1ltMUuqCvnB87tx97DLERGZkDED3d0HgNuAJ4kH9U/dfbOZ3WVm1wXNPmVmm83sVeBTwEemquCpYGZ86C3z2Hqog9/uOBJ2OSIiE2JhjUiXL1/udXV1obz3SHoHBrnim0+zqDKfH99yadjliIiMyMzWu/vykZ5L2TNFT5Sdkc4tVyzgue1HeHVfa9jliIicMgV6gg+snEdhLIPvPbMj7FJERE6ZAj1BfnYGH75sPk++fojtDZ1hlyMickoU6Cf4yGXziWWk839/tS3sUkRETokC/QSz8rP56OUL+M9XD7Cpvi3sckRExk2BPoJb37aQktxMvvnE1rBLEREZNwX6CApjmdx21SLWbmti7baZcYkCEZGxKNBH8WeX1jKnOIdvPL6VwSGdPSoiM58CfRTZGencsWoxmw+088CLuhKjiMx8CvSTeM8FVVx2xiy+9cRWGjt6x/4BEZEQKdBPwsy46/rz6Okf5OuPJ9X1xkQkBSnQx3BmRT4fu2Ihj7xcz/O6cJeIzGAK9HH4q6sWUVuayxce3qgvwRCRGUuBPg45Wel8+08vZF9LN994XMemi8jMpEAfpxULSvnzyxbwwxf28Nz2prDLERH5PQr0U/D5a85mYVken/v3V2nt7gu7HBGR4yjQT0FOVjrfuXEpTZ29fO7fN+rr6kRkRlGgn6IL5hbzxVXn8NSWw/zbc7vDLkdEZJgCfQL+/K3z+YNzKvn641t4ZW9L2OWIiAAK9AkxM779pxcwuyjGX/5wPYfbe8IuSUREgT5RxblZfP9Dy+nsHeDWH66np38w7JJEJMUp0E/D4tmF/N0NS3l1Xyt3PLyRIV2VUURCpEA/TdeeN5vP/eFZ/HzDAX0hhoiEKiPsAqLgk+84k8PtvdyzZiflBdnccsXCsEsSkRSkQJ8EZsad151LU2cvX/3FFnKzMvjAytqwyxKRFKNAnyTpacbfv38pPf3r+eufvcagOx+8dF7YZYlICtEc+iSKZabzzx+8mD84p4L/+fNN3P/b3WGXJCIpRIE+ybIz0vmnmy/mnUsq+ZvVm/mXtTvDLklEUoQCfQpkZaTxTzcvY9V5s/nqL7bw9ce26JBGEZlyCvQpkpmexnc/sIwPXjqPe9bs5NM/2aCTj0RkSmmn6BRKTzPuuv5c5pTk8I3Ht7K/pZt7/uxiKgpjYZcmIhGkEfoUMzM+/rYz+N7Ny9h6sIPrvvscG/e3hl2WiESQAn2arDq/ioc/cRnpacaf/vPzPLqhPuySRCRiFOjTaEl1IY/e9lYunFvMpx/awF//7DXNq4vIpFGgT7Oy/Gx+/LGVfPxtZ/DAur1c991nefNwR9hliUgEKNBDkJmexh2rFnP/X6yguauP6777LD98frcObRSR06JAD9HbzirnsU9fwSXzS/mfj27mg/etY39Ld9hliUiSUqCHrKIgxg/+YgVfe995bNjbyjV/v4Yfr9uj0bqInDIF+gxgZty8ch5PfOZKltYW86WfbeKGe55ny8H2sEsTkSQyrkA3s2vN7A0z225md5yk3Z+YmZvZ8skrMXXUlObyo4+u5Ft/cgE7Gjt5zz8+y1f/63U6ewfCLk1EksCYgW5m6cDdwCpgCXCTmS0ZoV0B8Glg3WQXmUrMjBsuqeHXt7+dG5bX8K/P7eKqbz/Dv9ft0zSMiJzUeEboK4Dt7r7T3fuAh4DrR2j3v4FvAj2TWF/KKsnL4ut/fD6PfOIyqotz+Px/bOS6u59l3c4jYZcmIjPUeAJ9DrAvYXl/sG6YmS0Datz9Fyd7ITO71czqzKyusbHxlItNRRfVlvDIJy7jO+9fypHOPt5/7wvccn8dmw+0hV2aiMwwp71T1MzSgL8Dbh+rrbvf6+7L3X15eXn56b51ykhLM/7oojn8+va38/lrzubFXUd49z88yyd+tF4nJYnIsPEEej1Qk7A8N1h3TAFwHvCMme0GLgVWa8fo5MvJSueT7ziTtV+4ik9ddSZrtzVxzXfW8KkHX2FHY2fY5YlIyMz95DvazCwDeBO4mniQvwR8wN03j9L+GeBz7l53stddvny519WdtImMoaWrj3vW7OT+3+6md2CQ915YzceuWMh5c4rCLk1EpoiZrXf3EQfMY14P3d0HzOw24EkgHbjP3Teb2V1AnbuvntxyZbxK8rK4Y9ViPnr5Au75zQ4eeHEvj244wFsWzuJjVy7g7WdVkJZmYZcpItNkzBH6VNEIffK1He3nwRf38v+e282h9h7OrMjnlssX8EcXzSGWmR52eSIyCU42QlegR1DfwBC/eO0A31+zi9cPtlOSm8kNy2u4aUUt88vywi5PRE6DAj1FuTvP7zjCD1/Ywy9fP8zgkHPFojJuXlnL1edUkpmuKz+IJBsFunC4vYefvLSPB1/cy8G2HioLs7lheQ1/vGwuCzRqF0kaCnQZNjA4xNNvNPKjF/awZlsj7rCstpj3LZvLey+oojg3K+wSReQkFOgyooNtR3l0wwEeeXk/bx7uJCs9jasWV/C+ZXN4x9kVZGVoSkZkplGgy0m5O5sPtPPIy/WsfrWeps4+SnIzee+F1bzngmounldCug5/FJkRFOgybgODQ6zd1sQjr9Tzy82H6B0Yoiw/i3cumc0151Zy2RllGrmLhEiBLhPS2TvAM2808MSmQzy9tYGuvkEKYhlcvbiCa8+bzZVnlZObNea5aSIyiRToctp6+gd5bnsTT2w6xH9vOUxrdz+xzDSuXFTONefO5m1nl1OWnx12mSKRd1qn/osAxDLTufqcSq4+p5KBwSFe3N3Mk5sO8cTmQ/zy9cMAnFtdyBWLyrlyURkXzy8hO0Nnp4pMJ43Q5bQMDTmv1bexdlsja7Y18fKeFgaGnJzMdFYuLOXKReVceVYZZ5TnY6YdqyKnS1MuMm06ewd4YccR1m5rZO22JnY2dQFQVRTjikVlXLGonLeeWUZpno53F5kIBbqEZl9zN89ub2Lttkae3dZEe88AZrB4diErF5SyYkEpl8wvpbxA8+8i46FAlxlhcMjZuL+VtduaWLfrCOv3tNDTPwTAwvK84YBfsWAWc4pzQq5WZGZSoMuM1DcwxKYDbby4q5kXdzXz0u5mOnoGAJhTnMNFtcUsqy3hotpillQXaierCAp0SRKDQ87WQ+3D4b5hbysH2noAyMpI47zqQi4KAn5ZbQlVRTHtaJWUo0CXpHWorYdX9rbwyr5WXtnbwsb9bfQOxKdpKguzWVpTzPlzijh/bvxeO1sl6nQcuiSt2UUxVp1fxarzq4D4NM3WQ+28sreVl4OAf3Lz4eH2c4pzgoAv4rw5RQp5SSkaoUvSa+/pZ1N9G5vq29i4P36/+0j38PNVRTEWzy7gnKrC4FbA/Fl5ZOgLPiQJaYQukVYYy+SyM8q47Iyy4XVtR/vZXN/Ga/VtbD3UwZaD7azd1sTAUHwAk52RxlmVBZxTVcDi2YUsrirgrMoCZuVlaV5ekpZG6JIyegcG2dHQxZaD7Ww91M6Wg/GgP9LVN9ymODeTRRX5nFmRz5kVBcOPtQNWZgqN0EWA7Ix0llQXsqS68Lj1DR09bD3YwfaGTrY1dLKjoZMnNh2ipXvfcJu8rPThkD8zCPmF5XnUlOTqcsIyYyjQJeVVFMSoKIhx5Vnlx60/0tnLtoSQ39bQwbPbG3n45f3DbdLTjLklOcyflceCsvhtflkeC8vyqC7O0ReDyLRSoIuMYlZ+NrPys7l04azj1rcd7Wd7Qye7m7rY1dTFriNd7G7q4qXdzXT3DQ63y0pPo6Y0hwVl+Swoy2V+WR7zZ+VRW5pLVVFMO2Vl0inQRU5RUU4mF88r4eJ5Jcetd3caO3rjIR8E/a7GLnYf6WLNtkb6guPnIT6yryqKUVOSS21pLjWlOdSU5jI3WC7L185ZOXUKdJFJYmZUFMaoKIyx8oRR/dCQc6DtKHuPdLOvpZt9zUfZ2xx//KutDTR19h7XPicznbkl8ZCvLo5RVZTDnOIcqopiVBfnMLsoRqZG+HICBbrINEhLM+aWxEfgIznaN8j+lu54yDd3s6/l6PD9y3tbaO3uP669GVQUZP9e0Mdv8cc6BDP1KNBFZoCcrHQWVRawqLJgxOe7+wY40NrDwbajHGg9yoHWnvh921G2HGznqS2Hhy+JcExWRhrVRfHRfWVhNpVFMSoLYlQWxuLLhTEqCrN10bMIUaCLJIHcrIzhwyVH4u60dPcHYR+/HWzroT64r9vTQkN7L32DQ7/3syW5mUG4x6gsyGZ20e8ex8M/Rll+lnbiJgEFukgEmBmleVmU5mVx3pyiEdu4O63d/Rzu6OFQWw8N7b0cbu/hcEcPh9t7aWjv4Y1D7TR29DJ0wvmGaQZl+dnxW0E2ZflZwXL8flbwuDw/m9I8hX9YFOgiKcLMKMnLoiQvi8WzC0dtNzjkHOns5XBi4LfFQ7+pM37b0dBJY2fvcUfuJCrJzRz+AJgVhH55QTazgg+d0qCO0twsCnMydbz+JFGgi8hx0tN+d7TO+Yw82of4iL+zd4Cmzj6aOns50tlLY2cfTR29wXJ8/ab6No509tHROzDi65hBcU5m/MMmN34rzcuMPw5Cvzg3c/hDoCQ3iyJ9CIxIgS4iE2JmFMQyKYhlsqAsb8z2Pf2DNHX20tLVT3N3Hy1dfbQE983dfbR099PS1cf+lm421cfbjPYXwPCHQG5WwgdBYuhnBh8MWRQH96nwIaBAF5FpEctMDw7dHF97d+do/yDNXX20dPXHw7+7L74chH9zdx+t3X3Utx5lU33bmB8CRTmZFOVkUhgL7nMyKIxlUpiTSWEsI1iXGazLSGiXSXZG2ow/DFSBLiIzkpmRm5VBblbGhD4EWrv7g/A/9ldA/EOgvaef9qP9tB3t51B7D+1H+2nv6R/+wvLRZKWnHf8BEHwIFI7jA6IgljEth4cq0EUkMibyIXBM78AgHT0DtB3tD0J+YDjs4+sGhj8M2oN2+5u7h5/vHzz5pcizMtLiYR/L4DPvPIvrLqw+jZ6OTIEuIkL88srZ+emU5Wef8s+6O70DQ8Mj/3jwDwyHfcfwh0N8XWnu1HwtogJdROQ0mRmxzHRimelUFMZCq0NH/4uIRMS4At3MrjWzN8xsu5ndMcLzHzez18xsg5k9a2ZLJr9UERE5mTED3czSgbuBVcAS4KYRAvsBdz/f3ZcC3wL+brILFRGRkxvPCH0FsN3dd7p7H/AQcH1iA3dvT1jMA8L55mkRkRQ2np2ic4B9Ccv7gZUnNjKzTwKfBbKAq0Z6ITO7FbgVoLa29lRrFRGRk5i0naLufre7nwF8AfjyKG3udffl7r68vLx8pCYiIjJB4wn0eqAmYXlusG40DwF/dBo1iYjIBIwn0F8CFpnZAjPLAm4EVic2MLNFCYvvBrZNXokiIjIeY86hu/uAmd0GPAmkA/e5+2Yzuwuoc/fVwG1m9gdAP9ACfHis112/fn2Tme2ZYN1lQNMEfzZZpWKfITX7rT6nhon2ed5oT5h78h2QYmZ17r487DqmUyr2GVKz3+pzapiKPutMURGRiFCgi4hERLIG+r1hFxCCVOwzpGa/1efUMOl9Tso5dBER+X3JOkIXEZETKNBFRCIi6QJ9rEv5RoWZ7U64JHFdsK7UzP7bzLYF96f4JVszi5ndZ2YNZrYpYd2IfbS4fwi2+0YzWxZe5RM3Sp/vNLP6YFtvMLN3JTz3xaDPb5jZNeFUfXrMrMbMnjaz181ss5l9Olgf2W19kj5P7bZ296S5ET+xaQewkPhFwF4FloRd1xT1dTdQdsK6bwF3BI/vAL4Zdp2n2ccrgWXAprH6CLwLeBww4FJgXdj1T2Kf7wQ+N0LbJcG/8WxgQfBvPz3sPkygz1XAsuBxAfBm0LfIbuuT9HlKt3WyjdDHvJRvxF0P3B88vp8kv2aOu68Bmk9YPVofrwd+4HEvAMVmVjUthU6iUfo8muuBh9y91913AduJ/x9IKu5+0N1fDh53AFuIX8U1stv6JH0ezaRs62QL9JEu5XuyX1Iyc+CXZrY+uOwwQKW7HwweHwIqwyltSo3Wx6hv+9uC6YX7EqbSItdnM5sPXASsI0W29Ql9hinc1skW6KnkcndfRvyboj5pZlcmPunxv9MifcxpKvQx8D3gDGApcBD421CrmSJmlg88DHzGj/9SnMhu6xH6PKXbOtkC/VQv5Zu03L0+uG8Afkb8z6/Dx/70DO4bwqtwyozWx8hue3c/7O6D7j4EfJ/f/akdmT6bWSbxYPuxuz8SrI70th6pz1O9rZMt0Me8lG8UmFmemRUcewz8IbCJeF+PXcnyw8Cj4VQ4pUbr42rgQ8EREJcCbQl/rie1E+aH30d8W0O8zzeaWbaZLQAWAS9Od32ny8wM+Fdgi7snft9wZLf1aH2e8m0d9t7gCew9fhfxPcY7gC+FXc8U9XEh8T3erwKbj/UTmAX8ivj15p8CSsOu9TT7+SDxPzv7ic8ZfnS0PhI/4uHuYLu/BiwPu/5J7PMPgz5tDP5jVyW0/1LQ5zeAVWHXP8E+X058OmUjsCG4vSvK2/okfZ7Sba1T/0VEIiLZplxERGQUCnQRkYhQoIuIRIQCXUQkIhToIiIRoUAXEYkIBbqISET8f1rOt4XGTCTmAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reshaped VERSION WITH Y SIGNAL"
      ],
      "metadata": {
        "id": "XjYJzj5kZEvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a ff dense\n",
        "class FFDense(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom ForwardForward-enabled Dense layer. It has an implementation of the\n",
        "    Forward-Forward network internally for use.\n",
        "    This layer must be used in conjunction with the `FFNetwork` model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        units,\n",
        "        optimizer,\n",
        "        loss_metric,\n",
        "        num_epochs=50,\n",
        "        use_bias=True,\n",
        "        kernel_initializer=\"glorot_uniform\",\n",
        "        bias_initializer=\"zeros\",\n",
        "        kernel_regularizer=None,\n",
        "        bias_regularizer=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = keras.layers.Dense(\n",
        "            units=units,\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=kernel_regularizer,\n",
        "            bias_regularizer=bias_regularizer,\n",
        "        )\n",
        "        self.relu = keras.layers.ReLU()\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_metric = loss_metric\n",
        "        self.threshold = 1.5\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "    # We perform a normalization step before we run the input through the Dense\n",
        "    # layer.\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        x_norm = tf.norm(x, ord=2, axis=1, keepdims=True)\n",
        "        x_dir = x / (x_norm + 1e-4)\n",
        "        res = self.dense(x_dir)\n",
        "        return tf.nn.relu(res)\n",
        "\n",
        "    # The Forward-Forward algorithm is below. We first perform the Dense-layer\n",
        "    # operation and then get a Mean Square value for all positive and negative\n",
        "    # samples respectively.\n",
        "    # The custom loss function finds the distance between the Mean-squared\n",
        "    # result and the threshold value we set (a hyperparameter) that will define\n",
        "    # whether the prediction is positive or negative in nature. Once the loss is\n",
        "    # calculated, we get a mean across the entire batch combined and perform a\n",
        "    # gradient calculation and optimization step. This does not technically\n",
        "    # qualify as backpropagation since there is no gradient being\n",
        "    # sent to any previous layer and is completely local in nature.\n",
        "\n",
        "    @tf.function\n",
        "    def forward_forward(self, x, y):\n",
        "        for i in range(self.num_epochs):\n",
        "            with tf.GradientTape() as tape:\n",
        "                xx = tf.reduce_mean(tf.square(self.call(x)), axis=1)\n",
        "                r = y * (self.threshold - xx) + (1 - y) * (xx - self.threshold)\n",
        "                loss = tf.math.log(1 + tf.math.exp(r))\n",
        "                mean_loss = tf.reduce_mean(loss)\n",
        "                self.loss_metric.update_state(mean_loss)\n",
        "            gradients = tape.gradient(mean_loss, self.dense.trainable_weights)\n",
        "            self.optimizer.apply_gradients(zip(gradients, self.dense.trainable_weights))\n",
        "        return tf.stop_gradient(self.call(x)), self.loss_metric.result()\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "kxY8DZPDZE2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a full ff network\n",
        "class FFNetwork(keras.Model):\n",
        "    \"\"\"\n",
        "    A [`keras.Model`](/api/models/model#model-class) that supports a `FFDense` network creation. This model\n",
        "    can work for any kind of classification task. It has an internal\n",
        "    implementation with some details specific to the MNIST dataset which can be\n",
        "    changed as per the use-case.\n",
        "    \"\"\"\n",
        "\n",
        "    # Since each layer runs gradient-calculation and optimization locally, each\n",
        "    # layer has its own optimizer that we pass. As a standard choice, we pass\n",
        "    # the `Adam` optimizer with a default learning rate of 0.03 as that was\n",
        "    # found to be the best rate after experimentation.\n",
        "    # Loss is tracked using `loss_var` and `loss_count` variables.\n",
        "    # Use legacy optimizer for Layer Optimizer to fix issue\n",
        "    # https://github.com/keras-team/keras-io/issues/1241\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dims,\n",
        "        layer_optimizer=keras.optimizers.legacy.Adam(learning_rate=0.03),\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layer_optimizer = layer_optimizer\n",
        "        self.loss_var = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
        "        self.loss_count = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
        "        self.layer_list = [keras.Input(shape=(dims[0],))]\n",
        "        for d in range(len(dims) - 1):\n",
        "            self.layer_list += [\n",
        "                FFDense(\n",
        "                    dims[d + 1],\n",
        "                    optimizer=self.layer_optimizer,\n",
        "                    loss_metric=keras.metrics.Mean(),\n",
        "                )\n",
        "            ]\n",
        "\n",
        "    # This function makes a dynamic change to the image wherein the labels are\n",
        "    # put on top of the original image (for this example, as MNIST has 10\n",
        "    # unique labels, we take the top-left corner's first 10 pixels). This\n",
        "    # function returns the original data tensor with the first 10 pixels being\n",
        "    # a pixel-based one-hot representation of the labels.\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def overlay_y_on_x(self, data):\n",
        "        X_sample, y_sample = data\n",
        "        max_sample = tf.reduce_max(X_sample, axis=0, keepdims=True)\n",
        "        max_sample = tf.cast(max_sample, dtype=tf.float64)\n",
        "        X_zeros = tf.zeros([10], dtype=tf.float64)\n",
        "        X_update = xla.dynamic_update_slice(X_zeros, max_sample, [y_sample])\n",
        "        X_sample = xla.dynamic_update_slice(X_sample, X_update, [0])\n",
        "        return X_sample, y_sample\n",
        "\n",
        "    # A custom `predict_one_sample` performs predictions by passing the images\n",
        "    # through the network, measures the results produced by each layer (i.e.\n",
        "    # how high/low the output values are with respect to the set threshold for\n",
        "    # each label) and then simply finding the label with the highest values.\n",
        "    # In such a case, the images are tested for their 'goodness' with all\n",
        "    # labels.\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def predict_one_sample(self, x):\n",
        "        goodness_per_label = []\n",
        "        x = tf.reshape(x, [tf.shape(x)[0] * tf.shape(x)[1]])\n",
        "        for label in range(10):\n",
        "            h, label = self.overlay_y_on_x(data=(x, label))\n",
        "            h = tf.reshape(h, [-1, tf.shape(h)[0]])\n",
        "            goodness = []\n",
        "            for layer_idx in range(1, len(self.layer_list)):\n",
        "                layer = self.layer_list[layer_idx]\n",
        "                h = layer(h)\n",
        "                goodness += [tf.math.reduce_mean(tf.math.pow(h, 2), 1)]\n",
        "            goodness_per_label += [\n",
        "                tf.expand_dims(tf.reduce_sum(goodness, keepdims=True), 1)\n",
        "            ]\n",
        "        goodness_per_label = tf.concat(goodness_per_label, 1)\n",
        "        return tf.cast(tf.argmax(goodness_per_label, 1), tf.float64)\n",
        "\n",
        "    def predict(self, data):\n",
        "        x = data\n",
        "        preds = list()\n",
        "        preds = tf.map_fn(fn=self.predict_one_sample, elems=x)\n",
        "        return np.asarray(preds, dtype=int)\n",
        "\n",
        "    # This custom `train_step` function overrides the internal `train_step`\n",
        "    # implementation. We take all the input image tensors, flatten them and\n",
        "    # subsequently produce positive and negative samples on the images.\n",
        "    # A positive sample is an image that has the right label encoded on it with\n",
        "    # the `overlay_y_on_x` function. A negative sample is an image that has an\n",
        "    # erroneous label present on it.\n",
        "    # With the samples ready, we pass them through each `FFLayer` and perform\n",
        "    # the Forward-Forward computation on it. The returned loss is the final\n",
        "    # loss value over all the layers.\n",
        "\n",
        "    @tf.function(jit_compile=True)\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        # Flatten op\n",
        "        x = tf.reshape(x, [-1, tf.shape(x)[1] * tf.shape(x)[2]])\n",
        "\n",
        "        x_pos, _ = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, y))\n",
        "\n",
        "        random_y = tf.random.shuffle(y)\n",
        "        x_neg, _ = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, random_y))\n",
        "\n",
        "        xx = tf.concat([x_pos, x_neg], axis=0)\n",
        "        yy = tf.concat([tf.ones(len(x_pos), dtype=tf.float32),\n",
        "                        tf.zeros(len(x_neg), dtype=tf.float32)], axis=0)\n",
        "\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            if isinstance(layer, FFDense):\n",
        "                print(f\"Training layer {idx+1} now : \")\n",
        "                xx, loss = layer.forward_forward(xx, yy)\n",
        "                self.loss_var.assign_add(loss)\n",
        "                self.loss_count.assign_add(1.0)\n",
        "            else:\n",
        "                print(f\"Passing layer {idx+1} now : \")\n",
        "                x = layer(x)\n",
        "        mean_res = tf.math.divide(self.loss_var, self.loss_count)\n",
        "        return {\"FinalLoss\": mean_res}\n"
      ],
      "metadata": {
        "id": "wZrJG9n1bFv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FFNetwork(dims=[784, 500, 500])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.03),\n",
        "    loss=\"mse\",\n",
        "    jit_compile=True,\n",
        "    metrics=[keras.metrics.Mean()],\n",
        ")\n",
        "\n",
        "epochs = 100\n",
        "history = model.fit(train_dataset, epochs=epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFb-IEKTdiBH",
        "outputId": "bad48eed-1c94-4f11-81a0-e0c0818317a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Training layer 1 now : \n",
            "Training layer 2 now : \n",
            "1/1 [==============================] - 65s 65s/step - FinalLoss: 0.7171\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.6886\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.6665\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.6414\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.6193\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.5994\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.5823\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5674\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.5545\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.5426\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5324\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.5231\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.5146\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.5069\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4999\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4935\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4875\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4819\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4766\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4716\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4670\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4626\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4585\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4546\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4510\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4476\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4444\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4413\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4383\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4355\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4327\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4301\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4276\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4252\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4228\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4205\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4184\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4164\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4144\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4124\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4106\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4088\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4072\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4055\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4039\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4023\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4008\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3993\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3979\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3965\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3952\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3938\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3925\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3913\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3901\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3889\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3877\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3866\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3855\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3844\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3833\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3823\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3812\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3803\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3793\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3783\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3774\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3764\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3755\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3747\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3738\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3729\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3721\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3713\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3705\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3697\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3689\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3681\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3673\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3666\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3658\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3651\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3644\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3637\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3630\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3623\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3616\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3610\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3603\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3597\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3591\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3584\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3578\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3572\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3566\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3560\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3554\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3548\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3543\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inference and testing\n",
        "preds = model.predict(tf.convert_to_tensor(x_test))\n",
        "\n",
        "preds = preds.reshape((preds.shape[0], preds.shape[1]))\n",
        "\n",
        "results = accuracy_score(preds, y_test)\n",
        "\n",
        "print(f\"Test Accuracy score : {results*100}%\")\n",
        "\n",
        "plt.plot(range(len(history.history[\"FinalLoss\"])), history.history[\"FinalLoss\"])\n",
        "plt.title(\"Loss over training\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "Ulz4-yE4dkNd",
        "outputId": "148a0a30-bf83-46d9-f760-8d8d73aaf4c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-435781520871>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# inference and testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## unsupervised\n"
      ],
      "metadata": {
        "id": "-sGV2N15I-hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# define a ff dense\n",
        "class FFDense(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom ForwardForward-enabled Dense layer. It has an implementation of the\n",
        "    Forward-Forward network internally for use.\n",
        "    This layer must be used in conjunction with the `FFNetwork` model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        units,\n",
        "        optimizer,\n",
        "        loss_metric,\n",
        "        num_epochs=50,\n",
        "        use_bias=True,\n",
        "        kernel_initializer=\"glorot_uniform\",\n",
        "        bias_initializer=\"zeros\",\n",
        "        kernel_regularizer=None,\n",
        "        bias_regularizer=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = keras.layers.Dense(\n",
        "            units=units,\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=kernel_regularizer,\n",
        "            bias_regularizer=bias_regularizer,\n",
        "        )\n",
        "        self.relu = keras.layers.ReLU()\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_metric = loss_metric\n",
        "        self.threshold = 1.5\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "    # We perform a normalization step before we run the input through the Dense\n",
        "    # layer.\n",
        "\n",
        "    #@tf.function\n",
        "    def call(self, x):\n",
        "        x_norm = tf.norm(x, ord=2, axis=1, keepdims=True)\n",
        "        x_dir = x / (x_norm + 1e-4)\n",
        "        res = self.dense(x_dir)\n",
        "        return tf.nn.relu(res)\n",
        "\n",
        "    # The Forward-Forward algorithm is below. We first perform the Dense-layer\n",
        "    # operation and then get a Mean Square value for all positive and negative\n",
        "    # samples respectively.\n",
        "    # The custom loss function finds the distance between the Mean-squared\n",
        "    # result and the threshold value we set (a hyperparameter) that will define\n",
        "    # whether the prediction is positive or negative in nature. Once the loss is\n",
        "    # calculated, we get a mean across the entire batch combined and perform a\n",
        "    # gradient calculation and optimization step. This does not technically\n",
        "    # qualify as backpropagation since there is no gradient being\n",
        "    # sent to any previous layer and is completely local in nature.\n",
        "\n",
        "    @tf.function\n",
        "    def mahalanobis_distance(self, x):\n",
        "        mean, var = tf.nn.moments(x, axes=0)\n",
        "        cov = tfp.stats.covariance(x, sample_axis=0, event_axis=-1)\n",
        "        inv_cov = tf.linalg.pinv(cov)\n",
        "        diff = x - mean\n",
        "        md = tf.reduce_sum(tf.matmul(diff, inv_cov) * diff, axis=1)\n",
        "        return md\n",
        "\n",
        "    @tf.function\n",
        "    def euclidean_distance(self, x):\n",
        "        center = tf.reduce_mean(x, axis=0)\n",
        "        distances = tf.norm(x - center, axis=1)\n",
        "        return distances\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def forward_forward(self, x, y=None):\n",
        "        for i in range(self.num_epochs):\n",
        "            with tf.GradientTape() as tape:\n",
        "                drive = self.call(x)\n",
        "                xx = tf.reduce_mean(tf.square(drive), axis=1)\n",
        "                if y == None:\n",
        "                  md = tf.math.abs(self.euclidean_distance(drive))\n",
        "                  y = 1. - md / tf.reduce_max(md)\n",
        "                r = y * (self.threshold - xx) + (1 - y) * (xx - self.threshold)\n",
        "                loss = tf.math.log(1 + tf.math.exp(r))\n",
        "                mean_loss = tf.reduce_mean(loss)\n",
        "                self.loss_metric.update_state(mean_loss)\n",
        "            gradients = tape.gradient(mean_loss, self.dense.trainable_weights)\n",
        "            self.optimizer.apply_gradients(zip(gradients, self.dense.trainable_weights))\n",
        "        return tf.stop_gradient(self.call(x)), self.loss_metric.result()"
      ],
      "metadata": {
        "id": "aPAwW8jpR_M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a full ff network\n",
        "class FFNetwork(keras.Model):\n",
        "    \"\"\"\n",
        "    A [`keras.Model`](/api/models/model#model-class) that supports a `FFDense` network creation. This model\n",
        "    can work for any kind of classification task. It has an internal\n",
        "    implementation with some details specific to the MNIST dataset which can be\n",
        "    changed as per the use-case.\n",
        "    \"\"\"\n",
        "\n",
        "    # Since each layer runs gradient-calculation and optimization locally, each\n",
        "    # layer has its own optimizer that we pass. As a standard choice, we pass\n",
        "    # the `Adam` optimizer with a default learning rate of 0.03 as that was\n",
        "    # found to be the best rate after experimentation.\n",
        "    # Loss is tracked using `loss_var` and `loss_count` variables.\n",
        "    # Use legacy optimizer for Layer Optimizer to fix issue\n",
        "    # https://github.com/keras-team/keras-io/issues/1241\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dims,\n",
        "        layer_optimizer=keras.optimizers.legacy.Adam(learning_rate=0.03),\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layer_optimizer = layer_optimizer\n",
        "        self.loss_var = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
        "        self.loss_count = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
        "        self.layer_list = [keras.Input(shape=(dims[0],))]\n",
        "        for d in range(len(dims) - 1):\n",
        "            self.layer_list += [\n",
        "                FFDense(\n",
        "                    dims[d + 1],\n",
        "                    optimizer=self.layer_optimizer,\n",
        "                    loss_metric=keras.metrics.Mean(),\n",
        "                )\n",
        "            ]\n",
        "\n",
        "    # This function makes a dynamic change to the image wherein the labels are\n",
        "    # put on top of the original image (for this example, as MNIST has 10\n",
        "    # unique labels, we take the top-left corner's first 10 pixels). This\n",
        "    # function returns the original data tensor with the first 10 pixels being\n",
        "    # a pixel-based one-hot representation of the labels.\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def overlay_y_on_x(self, data):\n",
        "        X_sample, y_sample = data\n",
        "        max_sample = tf.reduce_max(X_sample, axis=0, keepdims=True)\n",
        "        max_sample = tf.cast(max_sample, dtype=tf.float64)\n",
        "        X_zeros = tf.zeros([10], dtype=tf.float64)\n",
        "        X_update = xla.dynamic_update_slice(X_zeros, max_sample, [y_sample])\n",
        "        X_sample = xla.dynamic_update_slice(X_sample, X_update, [0])\n",
        "        return X_sample, y_sample\n",
        "\n",
        "    # A custom `predict_one_sample` performs predictions by passing the images\n",
        "    # through the network, measures the results produced by each layer (i.e.\n",
        "    # how high/low the output values are with respect to the set threshold for\n",
        "    # each label) and then simply finding the label with the highest values.\n",
        "    # In such a case, the images are tested for their 'goodness' with all\n",
        "    # labels.\n",
        "\n",
        "    def call(self, x):\n",
        "      x = tf.reshape(x, [-1, tf.shape(x)[0] * tf.shape(x)[1]])\n",
        "      for layer in self.layer_list:\n",
        "        x = layer(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def predict_one_sample(self, x):\n",
        "        goodness_per_label = []\n",
        "        x = tf.reshape(x, [tf.shape(x)[0] * tf.shape(x)[1]])\n",
        "        for label in range(10):\n",
        "            h, label = self.overlay_y_on_x(data=(x, label))\n",
        "            h = tf.reshape(h, [-1, tf.shape(h)[0]])\n",
        "            goodness = []\n",
        "            for layer_idx in range(1, len(self.layer_list)):\n",
        "                layer = self.layer_list[layer_idx]\n",
        "                h = layer(h)\n",
        "                goodness += [tf.math.reduce_mean(tf.math.pow(h, 2), 1)]\n",
        "            goodness_per_label += [\n",
        "                tf.expand_dims(tf.reduce_sum(goodness, keepdims=True), 1)\n",
        "            ]\n",
        "        goodness_per_label = tf.concat(goodness_per_label, 1)\n",
        "        return tf.cast(tf.argmax(goodness_per_label, 1), tf.float64)\n",
        "\n",
        "    def predict(self, data):\n",
        "        x = data\n",
        "        preds = list()\n",
        "        preds = tf.map_fn(fn=self.predict_one_sample, elems=x)\n",
        "        return np.asarray(preds, dtype=int)\n",
        "\n",
        "    # This custom `train_step` function overrides the internal `train_step`\n",
        "    # implementation. We take all the input image tensors, flatten them and\n",
        "    # subsequently produce positive and negative samples on the images.\n",
        "    # A positive sample is an image that has the right label encoded on it with\n",
        "    # the `overlay_y_on_x` function. A negative sample is an image that has an\n",
        "    # erroneous label present on it.\n",
        "    # With the samples ready, we pass them through each `FFLayer` and perform\n",
        "    # the Forward-Forward computation on it. The returned loss is the final\n",
        "    # loss value over all the layers.\n",
        "\n",
        "    @tf.function(jit_compile=True)\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        # Flatten op\n",
        "        x = tf.reshape(x, [-1, tf.shape(x)[1] * tf.shape(x)[2]])\n",
        "\n",
        "        x_pos, _ = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, y))\n",
        "\n",
        "        random_y = tf.random.shuffle(y)\n",
        "        x_neg, _ = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, random_y))\n",
        "\n",
        "        xx = tf.concat([x_pos, x_neg], axis=0)\n",
        "\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            if isinstance(layer, FFDense):\n",
        "                print(f\"Training layer {idx+1} now : \")\n",
        "                xx, loss = layer.forward_forward(xx, y=None)\n",
        "                self.loss_var.assign_add(loss)\n",
        "                self.loss_count.assign_add(1.0)\n",
        "            else:\n",
        "                print(f\"Passing layer {idx+1} now : \")\n",
        "                x = layer(x)\n",
        "        mean_res = tf.math.divide(self.loss_var, self.loss_count)\n",
        "        return {\"FinalLoss\": mean_res}\n",
        "#r = model.layers[0](tf.reshape(tf.convert_to_tensor(x_test[:128]), [-1, 28*28]))"
      ],
      "metadata": {
        "id": "V1IUEewcI-oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "model = FFNetwork(dims=[784, 500, 500])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.03),\n",
        "    loss=\"mse\",\n",
        "    jit_compile=True,\n",
        "    metrics=[keras.metrics.Mean()],\n",
        ")\n",
        "\n",
        "epochs = 2\n",
        "history = model.fit(train_dataset, epochs=epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llw1KETOJJXU",
        "outputId": "dfe65b2d-427f-495c-b63e-dbaf0c518e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "Training layer 1 now : \n",
            "Training layer 2 now : \n",
            "1/1 [==============================] - 64s 64s/step - FinalLoss: 0.4373\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 39s 39s/step - FinalLoss: 0.3870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.reshape(tf.convert_to_tensor(x_test[:128]), [-1, 28*28])\n",
        "\n",
        "r = model.layers[0](x)\n",
        "r"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eRo83vKkOQW",
        "outputId": "0d49fe0b-2e7d-4e5f-9aa4-8db7a5799233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(128, 500), dtype=float32, numpy=\n",
              "array([[1.2219908 , 4.391045  , 0.        , ..., 0.        , 0.06396344,\n",
              "        0.        ],\n",
              "       [2.7021596 , 4.301476  , 0.        , ..., 0.        , 0.43858445,\n",
              "        0.17932157],\n",
              "       [1.4623371 , 3.848335  , 1.0635747 , ..., 0.        , 0.6206826 ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [2.1090364 , 4.1114864 , 0.13470346, ..., 0.        , 1.2836194 ,\n",
              "        0.12334886],\n",
              "       [2.6825938 , 3.9775226 , 0.        , ..., 0.        , 0.05443571,\n",
              "        0.02138174],\n",
              "       [1.9457905 , 4.857208  , 0.        , ..., 0.        , 1.7707671 ,\n",
              "        0.        ]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inference and testing\n",
        "preds = model(tf.convert_to_tensor(x_test))\n",
        "\n",
        "preds = preds.reshape((preds.shape[0], preds.shape[1]))\n",
        "\n",
        "results = accuracy_score(preds, y_test)\n",
        "\n",
        "print(f\"Test Accuracy score : {results*100}%\")\n",
        "\n",
        "plt.plot(range(len(history.history[\"FinalLoss\"])), history.history[\"FinalLoss\"])\n",
        "plt.title(\"Loss over training\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "PjSDsP_2JNIs",
        "outputId": "b761557c-758b-4852-d4f7-d5d139fdb722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy score : 11.35%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}