{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNr0H9ZUDY3R1ciZ3LXeK90",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiGyt/snippets/blob/master/Forward_Forward_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import random\n",
        "from tensorflow.compiler.tf2xla.python import xla\n",
        "import tensorflow_probability as tfp\n"
      ],
      "metadata": {
        "id": "xWB5oPJP50RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "print(\"4 Random Training samples and labels\")\n",
        "idx1, idx2, idx3, idx4 = random.sample(range(0, x_train.shape[0]), 4)\n",
        "\n",
        "img1 = (x_train[idx1], y_train[idx1])\n",
        "img2 = (x_train[idx2], y_train[idx2])\n",
        "img3 = (x_train[idx3], y_train[idx3])\n",
        "img4 = (x_train[idx4], y_train[idx4])\n",
        "\n",
        "imgs = [img1, img2, img3, img4]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "for idx, item in enumerate(imgs):\n",
        "    image, label = item[0], item[1]\n",
        "    plt.subplot(2, 2, idx + 1)\n",
        "    plt.imshow(image, cmap=\"gray\")\n",
        "    plt.title(f\"Label : {label}\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZhpjSxae50pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preproc dataset\n",
        "\n",
        "x_train = x_train.astype(float) / 255\n",
        "x_test = x_test.astype(float) / 255\n",
        "y_train = y_train.astype(int)\n",
        "y_test = y_test.astype(int)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "\n",
        "train_dataset = train_dataset.batch(60000)\n",
        "test_dataset = test_dataset.batch(10000)\n"
      ],
      "metadata": {
        "id": "FMZIzsHE52Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a ff dense\n",
        "class FFDense(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom ForwardForward-enabled Dense layer. It has an implementation of the\n",
        "    Forward-Forward network internally for use.\n",
        "    This layer must be used in conjunction with the `FFNetwork` model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        units,\n",
        "        optimizer,\n",
        "        loss_metric,\n",
        "        num_epochs=50,\n",
        "        use_bias=True,\n",
        "        kernel_initializer=\"glorot_uniform\",\n",
        "        bias_initializer=\"zeros\",\n",
        "        kernel_regularizer=None,\n",
        "        bias_regularizer=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = keras.layers.Dense(\n",
        "            units=units,\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=kernel_regularizer,\n",
        "            bias_regularizer=bias_regularizer,\n",
        "        )\n",
        "        self.relu = keras.layers.ReLU()\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_metric = loss_metric\n",
        "        self.threshold = 1.5\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "    # We perform a normalization step before we run the input through the Dense\n",
        "    # layer.\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        x_norm = tf.norm(x, ord=2, axis=1, keepdims=True)\n",
        "        x_dir = x / (x_norm + 1e-4)\n",
        "        res = self.dense(x_dir)\n",
        "        return tf.nn.relu(res)\n",
        "\n",
        "    # The Forward-Forward algorithm is below. We first perform the Dense-layer\n",
        "    # operation and then get a Mean Square value for all positive and negative\n",
        "    # samples respectively.\n",
        "    # The custom loss function finds the distance between the Mean-squared\n",
        "    # result and the threshold value we set (a hyperparameter) that will define\n",
        "    # whether the prediction is positive or negative in nature. Once the loss is\n",
        "    # calculated, we get a mean across the entire batch combined and perform a\n",
        "    # gradient calculation and optimization step. This does not technically\n",
        "    # qualify as backpropagation since there is no gradient being\n",
        "    # sent to any previous layer and is completely local in nature.\n",
        "\n",
        "    @tf.function\n",
        "    def forward_forward(self, x, y):\n",
        "        for i in range(self.num_epochs):\n",
        "            with tf.GradientTape() as tape:\n",
        "                xx = tf.reduce_mean(tf.square(self.call(x)), axis=1)\n",
        "                r = y * (self.threshold - xx) + (1 - y) * (xx - self.threshold)\n",
        "                loss = tf.math.log(1 + tf.math.exp(r))\n",
        "                mean_loss = tf.reduce_mean(loss)\n",
        "                self.loss_metric.update_state(mean_loss)\n",
        "            gradients = tape.gradient(mean_loss, self.dense.trainable_weights)\n",
        "            self.optimizer.apply_gradients(zip(gradients, self.dense.trainable_weights))\n",
        "        return tf.stop_gradient(self.call(x)), self.loss_metric.result()\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "ydr0B0bA53WO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a full ff network\n",
        "class FFNetwork(keras.Model):\n",
        "    \"\"\"\n",
        "    A [`keras.Model`](/api/models/model#model-class) that supports a `FFDense` network creation. This model\n",
        "    can work for any kind of classification task. It has an internal\n",
        "    implementation with some details specific to the MNIST dataset which can be\n",
        "    changed as per the use-case.\n",
        "    \"\"\"\n",
        "\n",
        "    # Since each layer runs gradient-calculation and optimization locally, each\n",
        "    # layer has its own optimizer that we pass. As a standard choice, we pass\n",
        "    # the `Adam` optimizer with a default learning rate of 0.03 as that was\n",
        "    # found to be the best rate after experimentation.\n",
        "    # Loss is tracked using `loss_var` and `loss_count` variables.\n",
        "    # Use legacy optimizer for Layer Optimizer to fix issue\n",
        "    # https://github.com/keras-team/keras-io/issues/1241\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dims,\n",
        "        layer_optimizer=keras.optimizers.legacy.Adam(learning_rate=0.03),\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layer_optimizer = layer_optimizer\n",
        "        self.loss_var = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
        "        self.loss_count = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
        "        self.layer_list = [keras.Input(shape=(dims[0],))]\n",
        "        for d in range(len(dims) - 1):\n",
        "            self.layer_list += [\n",
        "                FFDense(\n",
        "                    dims[d + 1],\n",
        "                    optimizer=self.layer_optimizer,\n",
        "                    loss_metric=keras.metrics.Mean(),\n",
        "                )\n",
        "            ]\n",
        "\n",
        "    # This function makes a dynamic change to the image wherein the labels are\n",
        "    # put on top of the original image (for this example, as MNIST has 10\n",
        "    # unique labels, we take the top-left corner's first 10 pixels). This\n",
        "    # function returns the original data tensor with the first 10 pixels being\n",
        "    # a pixel-based one-hot representation of the labels.\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def overlay_y_on_x(self, data):\n",
        "        X_sample, y_sample = data\n",
        "        max_sample = tf.reduce_max(X_sample, axis=0, keepdims=True)\n",
        "        max_sample = tf.cast(max_sample, dtype=tf.float64)\n",
        "        X_zeros = tf.zeros([10], dtype=tf.float64)\n",
        "        X_update = xla.dynamic_update_slice(X_zeros, max_sample, [y_sample])\n",
        "        X_sample = xla.dynamic_update_slice(X_sample, X_update, [0])\n",
        "        return X_sample, y_sample\n",
        "\n",
        "    # A custom `predict_one_sample` performs predictions by passing the images\n",
        "    # through the network, measures the results produced by each layer (i.e.\n",
        "    # how high/low the output values are with respect to the set threshold for\n",
        "    # each label) and then simply finding the label with the highest values.\n",
        "    # In such a case, the images are tested for their 'goodness' with all\n",
        "    # labels.\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def predict_one_sample(self, x):\n",
        "        goodness_per_label = []\n",
        "        x = tf.reshape(x, [tf.shape(x)[0] * tf.shape(x)[1]])\n",
        "        for label in range(10):\n",
        "            h, label = self.overlay_y_on_x(data=(x, label))\n",
        "            h = tf.reshape(h, [-1, tf.shape(h)[0]])\n",
        "            goodness = []\n",
        "            for layer_idx in range(1, len(self.layer_list)):\n",
        "                layer = self.layer_list[layer_idx]\n",
        "                h = layer(h)\n",
        "                goodness += [tf.math.reduce_mean(tf.math.pow(h, 2), 1)]\n",
        "            goodness_per_label += [\n",
        "                tf.expand_dims(tf.reduce_sum(goodness, keepdims=True), 1)\n",
        "            ]\n",
        "        goodness_per_label = tf.concat(goodness_per_label, 1)\n",
        "        return tf.cast(tf.argmax(goodness_per_label, 1), tf.float64)\n",
        "\n",
        "    def predict(self, data):\n",
        "        x = data\n",
        "        preds = list()\n",
        "        preds = tf.map_fn(fn=self.predict_one_sample, elems=x)\n",
        "        return np.asarray(preds, dtype=int)\n",
        "\n",
        "\n",
        "    def call(self, x):\n",
        "      x = tf.reshape(tf.convert_to_tensor(x), [-1, 28*28])\n",
        "      for layer in self.layers:\n",
        "        x = layer(x)\n",
        "      return x\n",
        "      \n",
        "    # This custom `train_step` function overrides the internal `train_step`\n",
        "    # implementation. We take all the input image tensors, flatten them and\n",
        "    # subsequently produce positive and negative samples on the images.\n",
        "    # A positive sample is an image that has the right label encoded on it with\n",
        "    # the `overlay_y_on_x` function. A negative sample is an image that has an\n",
        "    # erroneous label present on it.\n",
        "    # With the samples ready, we pass them through each `FFLayer` and perform\n",
        "    # the Forward-Forward computation on it. The returned loss is the final\n",
        "    # loss value over all the layers.\n",
        "\n",
        "    @tf.function(jit_compile=True)\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        # Flatten op\n",
        "        x = tf.reshape(x, [-1, tf.shape(x)[1] * tf.shape(x)[2]])\n",
        "\n",
        "        x_pos, _ = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, y))\n",
        "\n",
        "        #random_y = tf.random.shuffle(y)\n",
        "        #x_neg, _ = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, random_y))\n",
        "        random_x = tf.random.shuffle(x_pos)\n",
        "        shape = tf.shape(x_pos)\n",
        "        indices = tf.cast(tf.random.uniform((shape[0], 2), 0, tf.cast(shape[1], tf.float32)), tf.int32)\n",
        "        dimrange = tf.range(shape[1])\n",
        "        mask = tf.logical_and(dimrange >= tf.reduce_min(indices, axis=1, keepdims=True),\n",
        "                                  dimrange < tf.reduce_max(indices, axis=1, keepdims=True))\n",
        "        mask = tf.cast(mask, tf.float64)\n",
        "        x_neg = mask * x_pos + (1. - mask) * random_x\n",
        "\n",
        "        xx = tf.concat([x_pos, x_neg], axis=0)\n",
        "        yy = tf.concat([tf.ones(len(x_pos), dtype=tf.float32),\n",
        "                        tf.zeros(len(x_neg), dtype=tf.float32)], axis=0)\n",
        "\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            if isinstance(layer, FFDense):\n",
        "                print(f\"Training layer {idx+1} now : \")\n",
        "                xx, loss = layer.forward_forward(xx, yy)\n",
        "                self.loss_var.assign_add(loss)\n",
        "                self.loss_count.assign_add(1.0)\n",
        "            else:\n",
        "                print(f\"Passing layer {idx+1} now : \")\n",
        "                x = layer(x)\n",
        "        mean_res = tf.math.divide(self.loss_var, self.loss_count)\n",
        "        return {\"FinalLoss\": mean_res}\n"
      ],
      "metadata": {
        "id": "Vl19J0vc54mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FFNetwork(dims=[784, 500, 500])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.03),\n",
        "    loss=\"mse\",\n",
        "    jit_compile=True,\n",
        "    metrics=[keras.metrics.Mean()],\n",
        ")\n",
        "\n",
        "epochs = 100\n",
        "history = model.fit(train_dataset, epochs=epochs)\n"
      ],
      "metadata": {
        "id": "u7WaKwHw55u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inference and testing\n",
        "preds = model.predict(tf.convert_to_tensor(x_test))\n",
        "\n",
        "preds = preds.reshape((preds.shape[0], preds.shape[1]))\n",
        "\n",
        "results = accuracy_score(preds, y_test)\n",
        "\n",
        "print(f\"Test Accuracy score : {results*100}%\")\n",
        "\n",
        "plt.plot(range(len(history.history[\"FinalLoss\"])), history.history[\"FinalLoss\"])\n",
        "plt.title(\"Loss over training\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zxrvhMIx57cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## unsupervised\n"
      ],
      "metadata": {
        "id": "ervB6ChF59Ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# define a ff dense\n",
        "class FFDense(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom ForwardForward-enabled Dense layer. It has an implementation of the\n",
        "    Forward-Forward network internally for use.\n",
        "    This layer must be used in conjunction with the `FFNetwork` model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        units,\n",
        "        optimizer,\n",
        "        loss_metric,\n",
        "        num_epochs=50,\n",
        "        use_bias=True,\n",
        "        kernel_initializer=\"glorot_uniform\",\n",
        "        bias_initializer=\"zeros\",\n",
        "        kernel_regularizer=None,\n",
        "        bias_regularizer=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = keras.layers.Dense(\n",
        "            units=units,\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=kernel_regularizer,\n",
        "            bias_regularizer=bias_regularizer,\n",
        "        )\n",
        "        self.relu = keras.layers.ReLU()\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_metric = loss_metric\n",
        "        self.threshold = 1.5\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "    # We perform a normalization step before we run the input through the Dense\n",
        "    # layer.\n",
        "\n",
        "    #@tf.function\n",
        "    def call(self, x):\n",
        "        x_norm = tf.norm(x, ord=2, axis=1, keepdims=True)\n",
        "        x_dir = x / (x_norm + 1e-4)\n",
        "        res = self.dense(x_dir)\n",
        "        return tf.nn.relu(res)\n",
        "\n",
        "    # The Forward-Forward algorithm is below. We first perform the Dense-layer\n",
        "    # operation and then get a Mean Square value for all positive and negative\n",
        "    # samples respectively.\n",
        "    # The custom loss function finds the distance between the Mean-squared\n",
        "    # result and the threshold value we set (a hyperparameter) that will define\n",
        "    # whether the prediction is positive or negative in nature. Once the loss is\n",
        "    # calculated, we get a mean across the entire batch combined and perform a\n",
        "    # gradient calculation and optimization step. This does not technically\n",
        "    # qualify as backpropagation since there is no gradient being\n",
        "    # sent to any previous layer and is completely local in nature.\n",
        "\n",
        "    @tf.function\n",
        "    def mahalanobis_distance(self, x):\n",
        "        mean, var = tf.nn.moments(x, axes=0)\n",
        "        cov = tfp.stats.covariance(x, sample_axis=0, event_axis=-1)\n",
        "        inv_cov = tf.linalg.pinv(cov)\n",
        "        diff = x - mean\n",
        "        md = tf.reduce_sum(tf.matmul(diff, inv_cov) * diff, axis=1)\n",
        "        return md\n",
        "\n",
        "    @tf.function\n",
        "    def euclidean_distance(self, x):\n",
        "        center = tf.reduce_mean(x, axis=0)\n",
        "        distances = tf.norm(x - center, axis=1)\n",
        "        return distances\n",
        "\n",
        "    \"\"\"\n",
        "    @tf.function\n",
        "    def forward_forward(self, x, y=None):\n",
        "        for i in range(self.num_epochs):\n",
        "            with tf.GradientTape() as tape:\n",
        "                drive = self.call(x)\n",
        "                xx = tf.reduce_mean(tf.square(drive), axis=1)\n",
        "                if y == None:\n",
        "                  md = self.euclidean_distance(drive)\n",
        "                  #y = 1. - md / tf.reduce_max(md)\n",
        "                  #y = tf.math.exp(-md)  # close to mean samples are \"real\"\n",
        "                  #y = 1. - tf.math.exp(-md)  # far from mean samples are \"real\"\n",
        "                  y = 1. - tf.nn.tanh(md)**2  # close to mean samples are \"real\"\n",
        "                  #y = tf.nn.tanh(md)**2 # far from mean samples are \"real\"\n",
        "                r = y * (self.threshold - xx) + (1 - y) * (xx - self.threshold)\n",
        "                loss = tf.math.log(1 + tf.math.exp(r))\n",
        "                mean_loss = tf.reduce_mean(loss)\n",
        "                #drive = tf.reduce_sum(tf.square(drive), axis=1)\n",
        "                #y_dist = drive / tf.reduce_sum(drive)\n",
        "                #mean_loss = 1./-tf.reduce_sum(y_dist * tf.math.log(y_dist))\n",
        "                self.loss_metric.update_state(mean_loss)\n",
        "            gradients = tape.gradient(mean_loss, self.dense.trainable_weights)\n",
        "            self.optimizer.apply_gradients(zip(gradients, self.dense.trainable_weights))\n",
        "        return tf.stop_gradient(self.call(x)), self.loss_metric.result()\n",
        "        \"\"\"\n",
        "\n",
        "    @tf.function\n",
        "    def forward_forward(self, x, y=None):\n",
        "        x = tf.cast(x, tf.float32)\n",
        "        if y == None:\n",
        "          shape = tf.shape(x)\n",
        "          y = tf.cast(tf.linspace(0, 1, shape[0]), tf.float32)\n",
        "          do_mask = tf.cast(tf.random.uniform(shape, 0, 1) > y[:, None], tf.float32)\n",
        "          x = x * do_mask\n",
        "\n",
        "        for i in range(self.num_epochs):\n",
        "            with tf.GradientTape() as tape:\n",
        "                drive = self.call(x)\n",
        "                xx = tf.reduce_mean(tf.square(drive), axis=1)\n",
        "                r = y * (self.threshold - xx) + (1 - y) * (xx - self.threshold)\n",
        "                loss = tf.math.log(1 + tf.math.exp(r))\n",
        "                mean_loss = tf.reduce_mean(loss)\n",
        "                self.loss_metric.update_state(mean_loss)\n",
        "            gradients = tape.gradient(mean_loss, self.dense.trainable_weights)\n",
        "            self.optimizer.apply_gradients(zip(gradients, self.dense.trainable_weights))\n",
        "        return tf.stop_gradient(self.call(x)), self.loss_metric.result()"
      ],
      "metadata": {
        "id": "nXjXrqOl59on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a full ff network\n",
        "class FFNetwork(keras.Model):\n",
        "    \"\"\"\n",
        "    A [`keras.Model`](/api/models/model#model-class) that supports a `FFDense` network creation. This model\n",
        "    can work for any kind of classification task. It has an internal\n",
        "    implementation with some details specific to the MNIST dataset which can be\n",
        "    changed as per the use-case.\n",
        "    \"\"\"\n",
        "\n",
        "    # Since each layer runs gradient-calculation and optimization locally, each\n",
        "    # layer has its own optimizer that we pass. As a standard choice, we pass\n",
        "    # the `Adam` optimizer with a default learning rate of 0.03 as that was\n",
        "    # found to be the best rate after experimentation.\n",
        "    # Loss is tracked using `loss_var` and `loss_count` variables.\n",
        "    # Use legacy optimizer for Layer Optimizer to fix issue\n",
        "    # https://github.com/keras-team/keras-io/issues/1241\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dims,\n",
        "        layer_optimizer=keras.optimizers.legacy.Adam(learning_rate=0.03),\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layer_optimizer = layer_optimizer\n",
        "        self.loss_var = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
        "        self.loss_count = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
        "        self.layer_list = [keras.Input(shape=(dims[0],))]\n",
        "        for d in range(len(dims) - 1):\n",
        "            self.layer_list += [\n",
        "                FFDense(\n",
        "                    dims[d + 1],\n",
        "                    optimizer=self.layer_optimizer,\n",
        "                    loss_metric=keras.metrics.Mean(),\n",
        "                )\n",
        "            ]\n",
        "\n",
        "    # This function makes a dynamic change to the image wherein the labels are\n",
        "    # put on top of the original image (for this example, as MNIST has 10\n",
        "    # unique labels, we take the top-left corner's first 10 pixels). This\n",
        "    # function returns the original data tensor with the first 10 pixels being\n",
        "    # a pixel-based one-hot representation of the labels.\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def overlay_y_on_x(self, data):\n",
        "        X_sample, y_sample = data\n",
        "        max_sample = tf.reduce_max(X_sample, axis=0, keepdims=True)\n",
        "        max_sample = tf.cast(max_sample, dtype=tf.float64)\n",
        "        X_zeros = tf.zeros([10], dtype=tf.float64)\n",
        "        X_update = xla.dynamic_update_slice(X_zeros, max_sample, [y_sample])\n",
        "        X_sample = xla.dynamic_update_slice(X_sample, X_update, [0])\n",
        "        return X_sample, y_sample\n",
        "\n",
        "    # A custom `predict_one_sample` performs predictions by passing the images\n",
        "    # through the network, measures the results produced by each layer (i.e.\n",
        "    # how high/low the output values are with respect to the set threshold for\n",
        "    # each label) and then simply finding the label with the highest values.\n",
        "    # In such a case, the images are tested for their 'goodness' with all\n",
        "    # labels.\n",
        "\n",
        "    def call(self, x):\n",
        "      x = tf.reshape(tf.convert_to_tensor(x), [-1, 28*28])\n",
        "      for layer in self.layers:\n",
        "        x = layer(x)\n",
        "      return x\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def predict_one_sample(self, x):\n",
        "        goodness_per_label = []\n",
        "        x = tf.reshape(x, [tf.shape(x)[0] * tf.shape(x)[1]])\n",
        "        for label in range(10):\n",
        "            h, label = self.overlay_y_on_x(data=(x, label))\n",
        "            h = tf.reshape(h, [-1, tf.shape(h)[0]])\n",
        "            goodness = []\n",
        "            for layer_idx in range(1, len(self.layer_list)):\n",
        "                layer = self.layer_list[layer_idx]\n",
        "                h = layer(h)\n",
        "                goodness += [tf.math.reduce_mean(tf.math.pow(h, 2), 1)]\n",
        "            goodness_per_label += [\n",
        "                tf.expand_dims(tf.reduce_sum(goodness, keepdims=True), 1)\n",
        "            ]\n",
        "        goodness_per_label = tf.concat(goodness_per_label, 1)\n",
        "        return tf.cast(tf.argmax(goodness_per_label, 1), tf.float64)\n",
        "\n",
        "    def predict(self, data):\n",
        "        x = data\n",
        "        preds = list()\n",
        "        preds = tf.map_fn(fn=self.predict_one_sample, elems=x)\n",
        "        return np.asarray(preds, dtype=int)\n",
        "\n",
        "    # This custom `train_step` function overrides the internal `train_step`\n",
        "    # implementation. We take all the input image tensors, flatten them and\n",
        "    # subsequently produce positive and negative samples on the images.\n",
        "    # A positive sample is an image that has the right label encoded on it with\n",
        "    # the `overlay_y_on_x` function. A negative sample is an image that has an\n",
        "    # erroneous label present on it.\n",
        "    # With the samples ready, we pass them through each `FFLayer` and perform\n",
        "    # the Forward-Forward computation on it. The returned loss is the final\n",
        "    # loss value over all the layers.\n",
        "\n",
        "    @tf.function(jit_compile=True)\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        # Flatten op\n",
        "        x = tf.reshape(x, [-1, tf.shape(x)[1] * tf.shape(x)[2]])\n",
        "\n",
        "        x_pos, _ = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, y))\n",
        "\n",
        "        random_y = tf.random.shuffle(y)\n",
        "        x_neg, _ = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, random_y))\n",
        "\n",
        "        xx = tf.concat([x_pos, x_neg], axis=0)\n",
        "\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            if isinstance(layer, FFDense):\n",
        "                print(f\"Training layer {idx+1} now : \")\n",
        "                xx, loss = layer.forward_forward(xx, y=None)\n",
        "                self.loss_var.assign_add(loss)\n",
        "                self.loss_count.assign_add(1.0)\n",
        "            else:\n",
        "                print(f\"Passing layer {idx+1} now : \")\n",
        "                x = layer(x)\n",
        "        mean_res = tf.math.divide(self.loss_var, self.loss_count)\n",
        "        return {\"FinalLoss\": mean_res}\n",
        "#r = model.layers[0](tf.reshape(tf.convert_to_tensor(x_test[:128]), [-1, 28*28]))"
      ],
      "metadata": {
        "id": "X6YycltX5_tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "model = FFNetwork(dims=[784, 500, 500])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.03),\n",
        "    loss=\"mse\",\n",
        "    jit_compile=True,\n",
        "    metrics=[keras.metrics.Mean()],\n",
        ")\n",
        "\n",
        "epochs = 10\n",
        "history = model.fit(train_dataset, epochs=epochs)\n"
      ],
      "metadata": {
        "id": "VgOzYjbn6A0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "\n",
        "x = x_test\n",
        "x = tf.reshape(tf.convert_to_tensor(x), [-1, 28*28])\n",
        "for layer in model.layers:\n",
        "  x = layer(x)\n",
        "train_score = silhouette_score(x, y_test)\n",
        "#train_score = silhouette_score(model(x_test), y_test)\n",
        "ref_score = silhouette_score(x_test.reshape([-1, 28*28]), y_test)\n",
        "\n",
        "print(f\"Silhouette Score\\nTrain: {train_score:.3f} vs Ref: {ref_score:.3f}\")"
      ],
      "metadata": {
        "id": "Gdd9SPzn6BG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# get preds\n",
        "preds = x[:512]\n",
        "#preds = model(x_test[:512])\n",
        "#preds = x_test.reshape([-1, 28*28])[:512]\n",
        "\n",
        "# get TSNE\n",
        "tsne = TSNE(\n",
        "    n_components=2,\n",
        "    init=\"random\",\n",
        "    random_state=0,\n",
        "    perplexity=100,\n",
        "    n_iter=300,\n",
        ")\n",
        "preds_tsne = tsne.fit_transform(preds)\n",
        "\n",
        "\n",
        "# plot all\n",
        "y_labels = np.unique(y_test)\n",
        "for i in y_labels:\n",
        "  #if i != 0:\n",
        "    idx = (y_test[:512] == i)\n",
        "    plt.scatter(preds_tsne[idx, 0], preds_tsne[idx, 1])\n",
        "plt.legend(y_labels)"
      ],
      "metadata": {
        "id": "DoUgzhty6CF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inference and testing\n",
        "preds = model.predict(tf.convert_to_tensor(x_test))\n",
        "\n",
        "preds = preds.reshape((preds.shape[0], preds.shape[1]))\n",
        "\n",
        "results = accuracy_score(preds, y_test)\n",
        "\n",
        "print(f\"Test Accuracy score : {results*100}%\")\n",
        "\n",
        "plt.plot(range(len(history.history[\"FinalLoss\"])), history.history[\"FinalLoss\"])\n",
        "plt.title(\"Loss over training\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "f-uSEbt56Df1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}